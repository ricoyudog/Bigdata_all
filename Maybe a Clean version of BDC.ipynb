{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "book = pd.read_csv('Train_data.csv',engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "testbook = pd.read_csv('Test_data.csv',engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "testbook.info(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#rename attribute\n",
    "book.rename(columns={'  num_pages':'num_pages'}, inplace = True)\n",
    "book['dataset'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#rename attribute\n",
    "testbook.rename(columns={'  num_pages':'num_pages'}, inplace = True)\n",
    "testbook['dataset'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#merge for feature engineery\n",
    "superbook = pd.concat([book,testbook], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "superbook.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#To see any repeat\n",
    "superbook.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "superbook.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#remove null attribute in the publisher columns\n",
    "superbookC = superbook[superbook['publisher'].notna()]\n",
    "book = book[book['publisher'].notna()]\n",
    "testbook = testbook[testbook['publisher'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#changing Three main format type\n",
    "\n",
    "superbookC['average_rating'] = superbookC['average_rating'].astype('float64')\n",
    "superbookC['text_reviews_count'] = superbookC['text_reviews_count'].astype('float64')\n",
    "#book['publication_date'] = pd.to_datetime(book['publication_date'],errors='coerce')\n",
    "\n",
    "book['average_rating'] = book['average_rating'].astype('float64')\n",
    "book['text_reviews_count'] = book['text_reviews_count'].astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#check the dataset label\n",
    "superbookC.loc[superbookC['dataset'] == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#change The date format\n",
    "years = superbookC['publication_date']\n",
    "year_type = years.unique()\n",
    "for year in years:\n",
    "    #print(type(year))\n",
    "    superbookC['publication_date'].replace(year, max(list(map(int,str(year).split(\"/\")))), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#change the date data type\n",
    "superbookC['publication_date'] = superbookC['publication_date'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#remove second translater.\n",
    "authors = superbookC['authors']\n",
    "authors_type = authors.unique()\n",
    "for author in authors:\n",
    "    superbookC['authors'].replace(author, author.split(\"/\")[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#romeve second translater for orgin dataset\n",
    "authors = book['authors']\n",
    "authors_type = authors.unique()\n",
    "for author in authors:\n",
    "    book['authors'].replace(author, author.split(\"/\")[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#end of classified rating\n",
    "#start of turning word in to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "superbookC['title'].head(20)\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "booktitle = []\n",
    "title = superbookC['title']\n",
    "for books in title:\n",
    "    booktitle.append(books)\n",
    "    \n",
    "    \n",
    "print(booktitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "\n",
    "for i, sentence in enumerate(booktitle):\n",
    "    word = re.sub('[(#)(()()]',' ',str(booktitle[i]))\n",
    "    word = nltk.word_tokenize(word)\n",
    "    booktitle[i] = word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(booktitle, workers = 1, size = 1, min_count = 1, window = 3, sg = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Vectorize_list = []\n",
    "for i in range(len(booktitle)):\n",
    "    model_list = []\n",
    "    for word in booktitle[i]:\n",
    "        model_list.append(model[word])\n",
    "   \n",
    "    Vectorize_list.append(sum(model_list)/len(model_list))\n",
    "    \n",
    "#end with making title into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#change vector datatype\n",
    "superbookC['Vectorize Title'] = Vectorize_list\n",
    "superbookC['Vectorize Title'] = superbookC['Vectorize Title'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#language进行分类，英语国家为1，非英语国家为0\n",
    "def language(language):\n",
    "    if language in ['eng','en-US','en-GB','en-CA','enm']:\n",
    "        return 1 #英语国家\n",
    "    else:\n",
    "        return 0 #非英语国家\n",
    "superbookC['language'] = superbookC['language_code'].map(language)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#author进行计数,即得到该作者写的书的总数\n",
    "author_type = book['authors'].unique()\n",
    "def author_bookscount(authors):\n",
    "    if authors in author_type:\n",
    "        counts = len(book.loc[book['authors'] == authors])\n",
    "        return counts\n",
    "    else:\n",
    "        return 0\n",
    "book['author_book_nums'] = book['authors'].map(author_bookscount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#处理num_pages的异常值\n",
    "#替换小于10页的\n",
    "num_mode = book['num_pages'].mode()\n",
    "def num_pages(pages):\n",
    "    if pages <= 10:\n",
    "        return 10\n",
    "    else:\n",
    "        return pages\n",
    "superbookC['num_pages'] = superbookC['num_pages'].map(num_pages)\n",
    "\n",
    "#处理ratingcounts=0时,average_rating仍有分数的情况\n",
    "superbookC.loc[superbookC['ratings_count'] == 0,'average_rating'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#make those book who is in series a dummy variable\n",
    "def if_series(title):\n",
    "    word = re.sub('[(#)(()()]',' ',str(title))\n",
    "    word = nltk.word_tokenize(word)\n",
    "    if_series = 0\n",
    "    for i in word:\n",
    "        if(i.isdigit() == True):\n",
    "            if_series = 1\n",
    "            break\n",
    "    return if_series\n",
    "\n",
    "superbookC['if_series'] = superbookC['title'].map(if_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#superbookC.loc[(superbookC['author_book_nums'] == 1) & (superbookC['if_series'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#对author进行处理\n",
    "##筛选出写作数量在五本以上的作家（共506）,同时将五本以下的人分为others,之后进行哑变量处理\n",
    "def author_group(authors):\n",
    "    author_group = book.loc[book['author_book_nums']>4,'authors'].unique()\n",
    "    if authors in author_group:\n",
    "        return authors\n",
    "    else:\n",
    "        return 'others'\n",
    "book['author_group'] = book['authors'].map(author_group)\n",
    "book['author_group'].unique()\n",
    "#五本以上的作者都转化为哑变量\n",
    "#author_group_dummy= pd.get_dummies(book['author_group'],prefix='author_group')\n",
    "#superbookC=pd.concat([book,author_group_dummy],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#開始對testdata進行處理\n",
    "author5 = book['author_group'].unique()\n",
    "#New column in Dataframe\n",
    "superbookC['author_group'] = 'others'\n",
    "superbookC.loc[superbookC['authors'].isin(author5) == True,'author_group'] = superbookC.loc[superbookC['authors'].isin(author5) == True]['authors']\n",
    "\n",
    "#五本以上的作者都转化为哑变量\n",
    "author_group_dummy= pd.get_dummies(superbookC['author_group'],prefix='author_group')\n",
    "superbookC=pd.concat([superbookC,author_group_dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#publisher进行计数,即得到该作者写的书的总数\n",
    "publisher_type = book['publisher'].unique()\n",
    "def publisher_bookscount(publisher):\n",
    "    if publisher in publisher_type:\n",
    "        counts = len(book.loc[book['publisher'] == publisher])\n",
    "        return counts\n",
    "    else:\n",
    "        return 0\n",
    "book['publisher_nums'] = book['publisher'].map(publisher_bookscount)\n",
    "\n",
    "#对publisher进行处理\n",
    "##筛选出出书数量在五本以上的出版社（共209）,同时将十本以下的出版社分为others,之后进行哑变量处理\n",
    "def publisher_group(publisher):\n",
    "    publisher_group = book.loc[book['publisher_nums']>=10,'publisher'].unique()\n",
    "    if publisher in publisher_group:\n",
    "        return publisher\n",
    "    else:\n",
    "        return 'others'\n",
    "book['publisher_group'] = book['publisher'].map(publisher_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#開始對testdata進行處理\n",
    "publisher5 = book['publisher_group'].unique()\n",
    "#New column in Dataframe\n",
    "superbookC['publisher_group'] = 'others'\n",
    "superbookC.loc[superbookC['publisher'].isin(publisher5) == True,'publisher_group'] = superbookC.loc[superbookC['publisher'].isin(publisher5) == True]['publisher']\n",
    "#十本以上的出版社都转化为哑变量\n",
    "publisher_group_dummy= pd.get_dummies(superbookC['publisher_group'],prefix='publisher_group')\n",
    "superbookC=pd.concat([superbookC,publisher_group_dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#split back into test and train\n",
    "Nbook = superbookC[superbookC['dataset'] == 0]\n",
    "NTestbook = superbookC[superbookC['dataset'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#end of vectorize the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('paper')\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = superbookC.groupby('language')['title'].count().plot.bar()\n",
    "plt.title('Language Code')\n",
    "plt.xticks(fontsize = 15)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x()-0.3, p.get_height()+100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.barplot(data=superbookC,x='language',y='average_rating')\n",
    "plt.xlabel('Languages')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.boxplot(data=superbookC,y='num_pages',x='language')\n",
    "plt.xlabel('Language')\n",
    "plt.ylabel('No. of Pages')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=superbookC['average_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#remove dummy variable\n",
    "data = Nbook.drop(Nbook.index[Nbook['num_pages'] >= 3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# prepare for the 3trial vairables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X = data.drop(['publication_date','authors','num_pages','average_rating','language_code', 'isbn','isbn13','title','bookID','text_reviews_count','author_group','publisher_group','dataset','publisher'], axis = 1)\n",
    "y = data['average_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "XB = Nbook.drop(['publication_date','title','authors','num_pages','average_rating','language_code', 'isbn','isbn13','title','bookID','text_reviews_count','author_group','publisher_group','dataset','publisher'], axis = 1)\n",
    "yB = Nbook['average_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "UploadX = NTestbook.drop(['publication_date','title','authors','num_pages','average_rating','language_code', 'isbn','isbn13','title','bookID','text_reviews_count','author_group','publisher_group','dataset','publisher'], axis = 1)\n",
    "Uploady = NTestbook['average_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# end of prepare for the 3trial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X.info(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# split 80% of the data to the training set and 20% of the data to test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X3_train, X3_test, y3_train, y3_test = train_test_split(XB, yB, test_size = 0.3, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#linear Regression\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "predictions = lr.predict(X3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pred = pd.DataFrame({'Actual': y3_test.tolist(), 'Predicted': predictions.tolist()}).head(25)\n",
    "pred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('MAE:', metrics.mean_absolute_error(y3_test, predictions))\n",
    "print('MSE:', metrics.mean_squared_error(y3_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y3_test, predictions)))\n",
    "print('score:',  explained_variance_score(y3_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Svm\n",
    "from sklearn import svm\n",
    "SVM = svm.SVR()\n",
    "SVM.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "SVMpredictions = SVM.predict(X3_test)\n",
    "pred = pd.DataFrame({'Actual': y3_test.tolist(), 'Predicted': SVMpredictions.tolist()}).head(25)\n",
    "print(pred.head(10))\n",
    "print('MSE:', metrics.mean_squared_error(y3_test, SVMpredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "\n",
    "kfold=StratifiedKFold(n_splits=10)\n",
    "\n",
    "RF_param_grid = {'n_estimators':[1000],\n",
    "                 'max_depth':[250],\n",
    "                 'random_state':[1]\n",
    "                    }\n",
    "\n",
    "RF=RandomForestRegressor()\n",
    "\n",
    "modelsRF = GridSearchCV(RF, param_grid=RF_param_grid, cv=5, scoring = 'neg_mean_squared_error',n_jobs = 6)\n",
    "\n",
    "modelsRF.fit(X_train,y_train)\n",
    "modelsRF_preds = modelsRF.predict(X3_test)\n",
    "#forest_model = RandomForestRegressor(random_state=1,n_estimators=160,max_depth = 80,max_features= 'log2')\n",
    "#forest_model.fit(X_train, y_train)\n",
    "#forest_preds = forest_model.predict(X_test)\n",
    "#print(mean_squared_error(y_test, modelsRF_preds))\n",
    "print(modelsRF.best_params_, mean_squared_error(y3_test, modelsRF_preds))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Randomforest Regressor result\n",
    "modelRF_Result = modelsRF.predict(UploadX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "testbook['average_rating'] = modelRF_Result\n",
    "testbook.to_csv('RandomForest_Result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Trial of Gradient Boost Regressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "GBR = GradientBoostingRegressor()\n",
    "\n",
    "RF_param_grid = {'n_estimators':[1000],\n",
    "                 'learning_rate':[0.01],\n",
    "                 'loss':['huber'],\n",
    "                 'max_depth':[10,100],\n",
    "                 'random_state':[1]\n",
    "                    }\n",
    "\n",
    "\n",
    "GBR_model = GridSearchCV(GBR, param_grid=RF_param_grid, cv=5, scoring = 'neg_mean_squared_error',n_jobs = -1)\n",
    "GBR_model.fit(X_train, y_train)\n",
    "GBR_preds = GBR_model.predict(X3_test)\n",
    "print(GBR_model.best_params_, mean_squared_error(y3_test, GBR_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#trial of AdaboostRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "AD = AdaBoostRegressor()\n",
    "print(AD.get_params().keys())\n",
    "AD_param_grid = {'n_estimators':[1000],\n",
    "                 'learning_rate':[0.01],\n",
    "                 'loss':['square'],\n",
    "                 'random_state':[1]\n",
    "                    }\n",
    "\n",
    "Ada_model = GridSearchCV(AD, param_grid=AD_param_grid, cv=5, scoring = 'neg_mean_squared_error',n_jobs = -1)\n",
    "Ada_model.fit(X_train, y_train)\n",
    "Ada_preds = Ada_model.predict(X3_test)\n",
    "print(Ada_model.best_params_, mean_squared_error(y3_test, Ada_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#trial of Bagging Regressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "Bag = BaggingRegressor()\n",
    "Bag_param_grid = {'n_estimators':[1000],\n",
    "                 'random_state':[1]}\n",
    "\n",
    "Bag_model = GridSearchCV(Bag, param_grid=Bag_param_grid, cv=5, scoring = 'neg_mean_squared_error',n_jobs = -1)\n",
    "Bag_model.fit(X_train, y_train)\n",
    "Bag_preds = Bag_model.predict(X3_test)\n",
    "print(Bag_model.best_params_, mean_squared_error(y3_test, Bag_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Bagging get the final result\n",
    "Bag_Result = Bag_model.predict(UploadX)\n",
    "print(Bag_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Bagging output to CSV\n",
    "testbook['average_rating'] = Bag_Result\n",
    "testbook.to_csv('Bag_Result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xg_reg = xgb.XGBRegressor()\n",
    "\n",
    "\n",
    "xg_param_grid = {'objective':['reg:tweedie'], 'colsample_bytree': [0.3], 'learning_rate':[0.01],\n",
    "'max_depth': [200,300], 'alpha':[0.5], 'n_estimators':[1000,1500],'lambda':[5],'seed':[1],'tweedie_variance_power':[1.8,1,3]}\n",
    "\n",
    "xg_model = GridSearchCV(xg_reg, param_grid=xg_param_grid, cv=5, scoring = 'neg_mean_squared_error',n_jobs = -1)\n",
    "\n",
    "xg_model.fit(X_train,y_train)\n",
    "xg_preds = xg_model.predict(X3_test)\n",
    "print(xg_model.best_params_,mean_squared_error(y3_test, xg_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "xg_result = xg_model.predict(UploadX)\n",
    "print(xg_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#xg output to csv\n",
    "testbook['average_rating'] = xg_result\n",
    "testbook.to_csv('xg_Result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "ridge = Ridge()\n",
    "lasso =Lasso()\n",
    "models = [modelsRF,Bag_model]\n",
    "\n",
    "\n",
    "params = {\n",
    "    'ridge__alpha': [0.01, 0.1, 0.5, 1, 3, 5, 7],\n",
    "    }\n",
    "sclf = StackingCVRegressor(regressors=models, meta_regressor=ridge, n_jobs = -1)\n",
    "# 训练回归器\n",
    "#grid = GridSearchCV(estimator=sclf, param_grid=params, cv=5, refit=True)\n",
    "sclf.fit(X_train, y_train)\n",
    "\n",
    "stack_pred = sclf.predict(X3_test)\n",
    "\n",
    "print(mean_squared_error(y3_test, stack_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#pytorch test area\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_train_torch = y_train.values\n",
    "y_train_torch = np.array(y_train_torch, dtype=np.float32)\n",
    "y_train_torch = y_train_torch.reshape(-1,1)\n",
    "y_train_tensor = Variable(torch.from_numpy(y_train_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_torch = X_train.values\n",
    "X_train_torch = np.array(X_train_torch, dtype=np.float32)\n",
    "#X_train_torch = X_train_torch.reshape(-1,1)\n",
    "X_train_tensor = Variable(torch.from_numpy(X_train_torch))\n",
    "len(X_train_tensor[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_test_torch = X3_test.values\n",
    "X_test_torch = np.array(X_test_torch, dtype=np.float32)\n",
    "#X_train_torch = X_train_torch.reshape(-1,1)\n",
    "X_test_tensor = Variable(torch.from_numpy(X_test_torch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Linear Regression with Pytorch\n",
    "\n",
    "# libraries\n",
    "import torch      \n",
    "from torch.autograd import Variable     \n",
    "import torch.nn as nn \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# create class\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        # super function. It inherits from nn.Module and we can access everythink in nn.Module\n",
    "        super(LinearRegression,self).__init__()\n",
    "        # Linear function.\n",
    "        self.linear = nn.Linear(input_dim,output_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# define model\n",
    "input_dim = 722\n",
    "output_dim = 1\n",
    "model = LinearRegression(input_dim,output_dim) # input and output size are 1\n",
    "\n",
    "# MSE\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Optimization (find parameters that minimize error)\n",
    "learning_rate = 0.01   # how fast we reach best parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
    "\n",
    "# train model\n",
    "loss_list = []\n",
    "iteration_number = 10000\n",
    "for iteration in range(iteration_number):\n",
    "    # optimization\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    # Forward to get output\n",
    "    results = model(X_train_tensor)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    loss = mse(results, y_train_tensor)\n",
    "    \n",
    "    # backward propagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # store loss\n",
    "    loss_list.append(loss.data)\n",
    "    \n",
    "    # print loss\n",
    "    if(iteration % 50 == 0):\n",
    "        print('epoch {}, loss {}'.format(iteration, loss.data))\n",
    "\n",
    "plt.plot(range(iteration_number),loss_list)\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "predicted = model(X_test_tensor).data.numpy()\n",
    "print('MSE:', metrics.mean_squared_error(y3_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, \n",
    "                                                y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, Batch_size, \n",
    "                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#nerual network\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "\n",
    "torch.manual_seed(1)    # reproducible\n",
    "\n",
    "x = X_train_tensor\n",
    "y = y_train_tensor             \n",
    "\n",
    "\n",
    "# 3 layer neural network\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,n_input,n_hidden,n_hidden1,n_output):\n",
    "        super(Net,self).__init__()\n",
    "        self.hidden=torch.nn.Linear(n_input,n_hidden)\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden,n_hidden1)\n",
    "        self.predict=torch.nn.Linear(n_hidden,n_output)\n",
    "\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X=F.relu(self.hidden(X))\n",
    "        X=F.relu(self.hidden2(X))\n",
    "        \n",
    "        #我猜這裡已經去掉了activation layer relu\n",
    "        X=self.predict(X)\n",
    "        return X\n",
    "    \n",
    "#seting parameter\n",
    "net=Net(n_input=722,n_hidden=500,n_hidden1=500,n_output=1)\n",
    "loss_func=torch.nn.MSELoss()\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "loss_List=[]\n",
    "epoch = 500\n",
    "\n",
    "for t in range(epoch):\n",
    "    for  i, (data, labels) in enumerate(train_loader):\n",
    "        prediction=net(data)\n",
    "        loss = mse(prediction, labels)\n",
    "        loss_List.append(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(epoch % 50 == 0):\n",
    "            print('epoch {}, loss {}'.format(t, loss.data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "predicted = net(X_test_tensor).data.numpy()\n",
    "print('MSE:', metrics.mean_squared_error(y3_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-b193db22",
   "language": "python",
   "display_name": "PyCharm (gcn-playground)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}